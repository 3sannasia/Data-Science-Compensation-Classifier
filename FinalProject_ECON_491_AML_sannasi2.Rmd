---
title: "| ECON 491 - Applied Machine Learning in Economics\n| Final Project\n"
author: "Akash Sannasi"
date: "12/04/2022"
output:
  html_document: default
  pdf_document: default
---


<!-- ```{r setup, include=FALSE} -->
<!-- knitr::opts_chunk$set(echo = TRUE) -->
<!-- ``` -->

---------------------------------------------------------

**Abstract:**
Since the start of the coronavirus pandemic, how have different data-related full-time U.S. jobs are structured salary-wise on the tier of salary based on remote-ratio, company size, position level, and other predictors is what I hope to find out. Essentially, I am figuring out which data jobs are classified to which salary tiers (below 100,000 U.S. dollars and greater than or equal to 100,000 U.S. dollars) to find their economic importance in the U.S. since the pandemic. This in terms of value on the job market as well as evaluate how company size, remote ratio, and skill level affect data job salary. I hope to better understand the growing demand for data jobs as students are wary of choosing such a field since they believe it is not financially secure. This is a question specific to the post-pandemic world that I am going to investigate as a classification problem. This will allow me to interpret data job trends and how the factors mentioned above affect potential salary and growth post-pandemic. I will use the specific data called the Economics of Data Careers on Kaggle by Jonathan Bown (https://www.kaggle.com/code/jonbown/economics-of-data-careers/data). I will refine it down to only including full-time jobs from the United States of America. The response variable will be the salary tier of either less than 100,000 (0) or greater than or equal to 100,000 (1). This is because students usually categorize a high salary by being six digits or more. For the purpose of this research paper, I will use the breakdown from the U.S. News as a reference. I will consider the lower tier as incomes 0 – 99, 000 U.S. dollars. I will consider the higher tier as 100,000 – infinity U.S. dollars. The nine predictors will be the work year (2020, 2021, 2022), experience level (EN, MI, SE, EX as dummy variables), remote ratio (0, 50, 100), company size (small, medium, large as dummy variables). The sample size is 328. I will conclude what the best model is out of a Logistic Regression Model, a K-NN model, and a Decision Tree Classifier model.

**Introduction:**
My research question is centered on how valuable full-time data are to companies since the pandemic in terms of salary depending on company size, remote ratio, and experience level. This will help me better understand the need for data-related skills for different companies and positions since the pandemic, where lot of data was needed to be processed. People should care if they are skeptical about whether a data job does not pay well or is not rising in value. People are also skeptical if remote ratio affects salary. In our research, we considered salary of 100,000 and above as high. This will hopefully instill more confidence in those wary of considering a data-related job because of job demand and salary. Machine learning is helpful in researching this question because I want to predict how remote ratio and company size can affect salary, which is why the data from 2020-2022 is useful. 

**Review of Literature:**
There has been another machine learning research paper called “Salary Prediction in Data Science Field Using Specialized Skills and Job Benefits” that aims to predict data science field salaries using specialized skills and job benefits as factors. They used random forests in order to have more stable and accurate prediction through the combination of multiple classifiers. Their focus also centered around data during and after the pandemic, which they call an industry revolution for how it changed and optimized jobs throughout the world to adapt to changing times. They claim there is a shortage of data science professionals and that through salary prediction for specifically data-related jobs they can allow students understand that those skills are valuable. The research paper essentially concluded that the best and most accurate classifying of salary actually depends on the type of data. They did not really interpret the data as they were more focused on finding the best model and deciding further testing is necessary.

Raheem, Mafas. Salary Prediction in Data Science Field Using Specialized Skills and ... July 2022, https://www.researchgate.net/publication/362280362_Salary_Prediction_in_Data_Science_Field_Using_Specialized_Skills_and_Job_Benefits_-A_Literature_Review. 

The news source has an article called “The Evolution of Data Science and AI at The New York Times” to highlight the increasing demand and necessity of data science jobs. They delve into how  New York Times has shifted to use artificial intelligence and data science in both the marketing and other business departments. This integration has only increased as businesses adapt to the availability of user data. They hold greater importance for data scientists because they want to know what data should be used to make marketing and business decisions. This includes analyzing risk and variability. The article basically argues that there is greater demand for data science skills in modern times because of how influential it can be in making business and marketing decisions. They also believe data science jobs are in demand and are necessary for businesses in the future because of the value they provide. 

Schmelzer, Ron. “The Evolution of Data Science and AI at the New York Times.” Forbes, Forbes Magazine, 21 Apr. 2022, https://www.forbes.com/sites/cognitiveworld/2021/10/09/creating-curating-and-optimizing-with-data-science-and-machine-learning-at-the-new-york-times/?sh=5e069be52020. 

**Empirical Application:**

Algorithm 1: Logistic Regression
```{r}
library(here)
library(dplyr)
library(caTools)
library('fastDummies')
set.seed(1)

salary_data_raw = read.csv(here("ds_salaries.csv"))

salary_data_unfiltered = salary_data_raw[, c( "work_year", "experience_level", "employment_type", "job_title", "salary_in_usd", "employee_residence", "company_size", "remote_ratio")]

salary_data = as.data.frame(salary_data_unfiltered)

#0 is less than 100,000 and 1 is >= 100,000
#Filtering out the data and making it ready for classification
salary_data$salary_in_usd <- ifelse(salary_data_unfiltered$salary_in_usd>=100000, 1, 0)
salary_data = subset(salary_data, employee_residence == "US" & employment_type  == "FT")
salary_data = salary_data[, c( "work_year", "experience_level", "salary_in_usd", "company_size", "remote_ratio")]

salary_data <- dummy_cols(salary_data, select_columns = 'company_size')
salary_data <- dummy_cols(salary_data, select_columns = 'experience_level')
salary_data <- dummy_cols(salary_data, select_columns = c('company_size','experience_level'),
           remove_selected_columns = TRUE)

head(salary_data)

#Filtered out uneccesary data

#Dimension of data
dim(salary_data)

split <- sample.split(salary_data, SplitRatio = 0.8)
   
train_reg <- subset(salary_data, split == "TRUE")
test_reg <- subset(salary_data, split == "FALSE")

logistic_model <- glm(salary_in_usd ~ ., 
                      data = train_reg, 
                      family = "binomial")
summary(logistic_model)

predict_reg <- predict(logistic_model, 
                       test_reg, type = "response")
predict_reg  
predict_reg = as.data.frame(predict_reg)
# Changing probabilities
predict_reg <- ifelse(predict_reg >0.5, 1, 0)
   
# Evaluating model accuracy
# using confusion matrix
table(test_reg$salary_in_usd, predict_reg)

err_rate <- mean(predict_reg != test_reg$salary_in_usd) * 100
err_rate
```


The error rate is about 22.73% using logistic regression on my dataset. Looking at the table, experience_level_EN and experience_level_MI are shown to be significant predictors of which salary tier the job should be placed in. 

Algorithm 2: K-NN Classifier
```{r}
library(dplyr)
library(caTools)
library(e1071)
library(caTools)
library(class)
library(here)
  
set.seed(1)
salary_data_raw = read.csv(here("ds_salaries.csv"))

salary_data_unfiltered = salary_data_raw[, c( "work_year", "experience_level", "employment_type", "job_title", "salary_in_usd", "employee_residence", "company_size", "remote_ratio")]

salary_data = as.data.frame(salary_data_unfiltered)

#0 is less than 100,000 and 1 is >= 100,000
#Filtering out the data and making it ready for classification
salary_data$salary_in_usd <- ifelse(salary_data_unfiltered$salary_in_usd>=100000, 1, 0)
salary_data = subset(salary_data, employee_residence == "US" & employment_type  == "FT")
salary_data = salary_data[, c( "work_year", "experience_level", "salary_in_usd", "company_size", "remote_ratio")]

salary_data <- dummy_cols(salary_data, select_columns = 'company_size')
salary_data <- dummy_cols(salary_data, select_columns = 'experience_level')
salary_data <- dummy_cols(salary_data, select_columns = c('company_size','experience_level'), remove_selected_columns = TRUE)

head(salary_data)
dim(salary_data)
sqrt(328)
#Optimal K is sometimes square root of total observations


split <- sample.split(salary_data, SplitRatio = 0.8)
train_cl <- subset(salary_data, split == "TRUE")
test_cl <- subset(salary_data, split == "FALSE")

classifier_knn.18 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 18)
classifier_knn.18
misClassError <- mean(classifier_knn.18 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.1 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 1)
classifier_knn.1
misClassError2 <- mean(classifier_knn.1 != test_cl$salary_in_usd) * 100
misClassError2

classifier_knn.2 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 2)

misClassError <- mean(classifier_knn.2 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.3 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 3)

misClassError <- mean(classifier_knn.3 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.4 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 4)

misClassError <- mean(classifier_knn.4 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.5 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 5)

misClassError <- mean(classifier_knn.5 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.6 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 6)

misClassError <- mean(classifier_knn.6 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.7 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 7)

misClassError <- mean(classifier_knn.7 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.8 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 8)

misClassError <- mean(classifier_knn.8 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.9 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 9)

misClassError <- mean(classifier_knn.9 != test_cl$salary_in_usd) * 100
misClassError

classifier_knn.10 <- knn(train = train_cl,
                      test = test_cl,
                      cl = train_cl$salary_in_usd,
                      k = 10)

misClassError <- mean(classifier_knn.10 != test_cl$salary_in_usd) * 100
misClassError
#Levels off then increases, so we will use K = 3
```


The error rate for the K-NN model with K = 3 is about 6.06%. This is a massive increase in efficiency compared to the logistic regression model. The optimal K was found through calculating the error rates from 1-10 and identifying which had the lowest error rate while not being overly flexible. A balanced model in terms of bias and variability was found at K = 3.

Algorithm 3: Decision Tree Classifier
```{r}
library(caTools)
library(party)
library(fastDummies)
library(magrittr)
library(dplyr)
library(datasets)
library(here)
set.seed(1)
salary_data_raw = read.csv(here("ds_salaries.csv"))


salary_data_unfiltered = salary_data_raw[, c( "work_year", "experience_level", "employment_type", "job_title", "salary_in_usd", "employee_residence", "company_size", "remote_ratio")]

salary_data = as.data.frame(salary_data_unfiltered)

#0 is less than 100,000 and 1 is >= 100,000
#Filtering out the data and making it ready for classification
salary_data$salary_in_usd <- ifelse(salary_data_unfiltered$salary_in_usd>=100000, 1, 0)
salary_data = subset(salary_data, employee_residence == "US" & employment_type  == "FT")
salary_data = salary_data[, c( "work_year", "experience_level", "salary_in_usd", "company_size", "remote_ratio")]

salary_data <- dummy_cols(salary_data, select_columns = 'company_size')
salary_data <- dummy_cols(salary_data, select_columns = 'experience_level')
salary_data <- dummy_cols(salary_data, select_columns = c('company_size','experience_level'),
           remove_selected_columns = TRUE)

head(salary_data)
salary_data$salary_in_usd = factor(salary_data$salary_in_usd,
                           levels = c(0, 1))

sample_data = sample.split(salary_data, SplitRatio = 0.8)
train_data <- subset(salary_data, sample_data == TRUE)
test_data <- subset(salary_data, sample_data == FALSE)
classifier<- ctree(salary_in_usd ~ ., train_data)
classifier
pred<-predict(classifier, test_data)
plot(classifier)
confusion_matrix = table(test_data$salary_in_usd, pred)
confusion_matrix
err_rate <- mean(pred != test_data$salary_in_usd) * 100
err_rate
```


The error rate of the decision tree classifier is about 19.70%. This is the second-lowest error rate out of the three models. Decision trees allow for easier interpretation of the data. As we can see in the tree, experience_level_EN and experience_level_MI are influential predictors in the model. There are three terminal nodes, with Node 3 having the most observations. In other words, those not in experience level EN nor experience level MI are more likely to have a salary of over 100,000. In this case, that means senior or executive roles. Therefore, in the data science field, there is room for growth and climbing up the ladder to higher salaries based on this model. In addition, about 61 of the observations out of 262 that the decision tree used were MI and had a greater chance of having a salary over 100,000.  

**Overall Interpretation:**

The error rate for logistic regression is about 22.73% The error rate for K-NN is about 6.06% with optimal K of 3. The error rate of the decision tree is about 19.70%. Although the K-NN model has the lowest error rate, the decision tree allows for better interpretability to answer our research question. Both the logistic regression and decision tree classifier models determined experience level of EN and MI as significant in predicting whether one had a salary of greater than 100,000 U.S. dollars or below 100,000 U.S. dollars. This proves that in the data science field, there is room to grow in the career ladder. One can achieve what we consider the high tier of salary over time, rather than being stagnant below 100k in the data science field. These results can help students confidentially choose this career. The logistic regression and decision tree classifier models also tell us that remote ratio and company size do not affect salary as much as the skill level of an employee. This still stays as a significant salary predictor in data science careers. This allows us to infer that career growth through that accumulation of skills still follows a positive trend in salary according to these models. People should not be skeptical whether the data science field cannot rise in value because an upward trend in salary is possible and also proven in data since 2020. 

**References**

Raheem, Mafas. Salary Prediction in Data Science Field Using Specialized Skills and ... July 2022, https://www.researchgate.net/publication/362280362_Salary_Prediction_in_Data_Science_Field_Using_Specialized_Skills_and_Job_Benefits_-A_Literature_Review. 

Schmelzer, Ron. “The Evolution of Data Science and AI at the New York Times.” Forbes, Forbes Magazine, 21 Apr. 2022, https://www.forbes.com/sites/cognitiveworld/2021/10/09/creating-curating-and-optimizing-with-data-science-and-machine-learning-at-the-new-york-times/?sh=5e069be52020. 

